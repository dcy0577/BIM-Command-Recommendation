{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1043bb0-c791-4302-941f-cc86a1075967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import string\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdeb899",
   "metadata": {},
   "source": [
    "### Filtering Functions: **Actual Modeling Logs** → **Aligned Logs**\n",
    "This process includes:\n",
    "\n",
    "- Translation of command names based on `command_dictionary.csv`.  \n",
    "- Redundant command removal based on `command_pairs_collections.csv`, retaining only high-level commands to represent the same action.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a2204fd-7107-4a09-ba5e-6d106e1ddd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_unique_commands(path):\n",
    "    df_unique_commands = pd.read_parquet(path)\n",
    "    df_unique_commands.reset_index(inplace=True)\n",
    "    return df_unique_commands\n",
    "\n",
    "def drop_less_commands(df, commands_set):\n",
    "    return df[~df['message'].isin(commands_set)]\n",
    "\n",
    "def read_language_dic(path, df):\n",
    "    language_df = pd.read_csv(path)\n",
    "    translation_dict = pd.Series(language_df.label.values, index=language_df.message).to_dict()\n",
    "    df['message_eng'] = df['message'].map(translation_dict)\n",
    "    return df.dropna(subset=['message_eng'])\n",
    "\n",
    "def contains_non_printable(text):\n",
    "    printable = set(string.printable)\n",
    "    return any(char not in printable for char in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4577e499-ba7a-411f-b2ab-b789bc23faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_mapping_dict(tool_menu_dict, tool_menu_event_path):\n",
    "    df_translation = pd.read_csv(tool_menu_event_path)\n",
    "    for index, row in df_translation.iterrows():\n",
    "        tool_menu_key = row['tool/menu']\n",
    "        # If the key does not exist in the dictionary, create a new list\n",
    "        if tool_menu_key not in tool_menu_dict:\n",
    "            tool_menu_dict[tool_menu_key] = []\n",
    "        # Append the 'Following_UNDO' to the list of the corresponding 'tool_menu'\n",
    "        tool_menu_dict[tool_menu_key].append(row['event'])\n",
    "\n",
    "def check_message(tool_menu_dict, tool_menu_key, undo_row) -> bool:\n",
    "    if tool_menu_key not in tool_menu_dict:\n",
    "        return False\n",
    "    else:\n",
    "        # Extract the 'message_eng' from the undo_row\n",
    "        undo_message = undo_row['message_eng']\n",
    "        # Check if the undo message is listed under the tool/menu key in the dictionary\n",
    "        if undo_message in tool_menu_dict[tool_menu_key]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "            \n",
    "def find_drop_rows(rows_to_drop, index, row, undo_rows, tool_menu_dict):\n",
    "\n",
    "    self_triggered = False\n",
    "    \n",
    "    # If no matching 'UNDO' row is found, check if the tool_menu_key exists in the dictionary\n",
    "    if row['message_eng'] not in tool_menu_dict:\n",
    "        self_triggered = True\n",
    "        \n",
    "    else:\n",
    "        # Iterate over the 'UNDO' rows\n",
    "        for _, undo_row in undo_rows.iterrows():\n",
    "            # Check if the 'UNDO' row's message matches the criteria for the 'Tool'/'Menu' event\n",
    "            if check_message(tool_menu_dict, row['message_eng'], undo_row):\n",
    "                # If a match is found, mark the 'UNDO' row for dropping\n",
    "                rows_to_drop.append(undo_row.name)\n",
    "                break  # No need to check further 'UNDO' rows\n",
    "\n",
    "\n",
    "    # If no matching 'UNDO' row is found and the event is not self-triggered, consider dropping the event row\n",
    "    if self_triggered is False and not any(check_message(tool_menu_dict, row['message_eng'], undo_row) for _, undo_row in undo_rows.iterrows()):\n",
    "        rows_to_drop.append(index)\n",
    "\n",
    "\n",
    "def replace_low_level(df, tool_menu_event_path):\n",
    "    tool_menu_dict = {}\n",
    "    get_mapping_dict(tool_menu_dict, tool_menu_event_path)  # Populate the tool_menu_dict\n",
    "\n",
    "    grouped_data = df.groupby('session_anonymized')\n",
    "    processed_data = []\n",
    "    for session_id, group_df in tqdm(grouped_data, desc=\"Processing grouped data filtering\"):\n",
    "        rows_to_drop = []\n",
    "        for index, row in reversed(list(group_df.iterrows())):\n",
    "            if row['cat'] in ['Tool', 'Menu']:\n",
    "                ts = row['ts']\n",
    "                # Define the range of indices for surrounding 40 rows\n",
    "                start_index = max(0, index - 10)\n",
    "                end_index = index + 30\n",
    "                # Use boolean indexing to filter rows within the desired range\n",
    "                surrounding_rows = group_df[(group_df.index >= start_index) & (group_df.index <= end_index)]\n",
    "                sub_rows = surrounding_rows[(surrounding_rows['ts'] >= ts) & ~surrounding_rows.index.isin(rows_to_drop)]\n",
    "                up_rows = surrounding_rows[(surrounding_rows['ts'] < ts) & ~surrounding_rows.index.isin(rows_to_drop)]\n",
    "\n",
    "                # Find the first 'UNDO' action in these subsequent rows\n",
    "                undo_rows = pd.concat([\n",
    "                    sub_rows[sub_rows['cat'] == 'UNDO'].head(5),\n",
    "                    up_rows[up_rows['cat'] == 'UNDO'].head(1)\n",
    "                ])\n",
    "                       \n",
    "                if not undo_rows.empty:\n",
    "                    find_drop_rows(rows_to_drop, index, row, undo_rows, tool_menu_dict)\n",
    "                else:\n",
    "                    rows_to_drop.append(index)\n",
    "        # Drop the rows that are to be filtered out\n",
    "        group_df = group_df.drop(rows_to_drop).reset_index(drop=True)\n",
    "        processed_data.append(group_df)\n",
    "\n",
    "    data = pd.concat(processed_data, ignore_index=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0c7aa22-98df-4100-93c7-d76dabd94436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path, unique_commands_path, lang_dict_path, tool_menu_event_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    df_unique_commands = read_unique_commands(unique_commands_path)\n",
    "    drop_commands = df_unique_commands[df_unique_commands['count'] <= 10]['message'].tolist()\n",
    "\n",
    "    df = drop_less_commands(df, drop_commands)\n",
    "    df = read_language_dic(lang_dict_path, df)\n",
    "    df = df[~df['message_eng'].apply(contains_non_printable)]\n",
    "\n",
    "    high_level_data = replace_low_level(df, tool_menu_event_path)\n",
    "    return high_level_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cd07662-12d0-41cf-b2db-66eca05240b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_in_parallel(file_paths, unique_commands_path, lang_dict_path, tool_menu_event_path, output_path, num_workers=80):\n",
    "    with Pool(processes=num_workers) as pool:\n",
    "        results = pool.starmap(process_file, [(file_path, unique_commands_path, lang_dict_path, tool_menu_event_path) for file_path in file_paths])\n",
    "    \n",
    "    combined_data = pd.concat(results, ignore_index=True)\n",
    "    combined_data.to_parquet(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee82af0-eb0b-4b75-a590-c926ba58bc53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing grouped data filtering: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n",
      "Processing grouped data filtering: 100%|██████████| 38/38 [00:16<00:00,  2.30it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:28:11<00:00,  1.89it/s]]    \n",
      "Processing grouped data filtering:  87%|████████▋ | 8746/9998 [1:28:30<13:24,  1.56it/s]] \n",
      "Processing grouped data filtering: 100%|██████████| 10000/10000 [1:29:11<00:00,  1.87it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:29:14<00:00,  1.87it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:29:31<00:00,  1.86it/s]] \n",
      "Processing grouped data filtering: 100%|██████████| 10000/10000 [1:29:47<00:00,  1.86it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:29:48<00:00,  1.86it/s]]  \n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:29:37<00:00,  1.86it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9993/9993 [1:30:31<00:00,  1.84it/s]]  \n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:31:12<00:00,  1.83it/s]]]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:31:28<00:00,  1.82it/s]] \n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:31:52<00:00,  1.81it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:32:03<00:00,  1.81it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:31:57<00:00,  1.81it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 10000/10000 [1:31:52<00:00,  1.81it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:32:17<00:00,  1.81it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 10000/10000 [1:32:26<00:00,  1.80it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 10000/10000 [1:32:31<00:00,  1.80it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:32:43<00:00,  1.80it/s]]]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:32:42<00:00,  1.80it/s]] \n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:32:54<00:00,  1.79it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:32:38<00:00,  1.80it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:32:38<00:00,  1.80it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9995/9995 [1:33:13<00:00,  1.79it/s]] \n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:33:32<00:00,  1.78it/s]] \n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:33:33<00:00,  1.78it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:33:35<00:00,  1.78it/s]\n",
      "Processing grouped data filtering:  98%|█████████▊| 9756/10000 [1:33:22<01:24,  2.88it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9995/9995 [1:33:43<00:00,  1.78it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:33:44<00:00,  1.78it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9996/9996 [1:33:37<00:00,  1.78it/s]] \n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:33:51<00:00,  1.78it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:33:38<00:00,  1.78it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:33:34<00:00,  1.78it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9996/9996 [1:33:58<00:00,  1.77it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:34:02<00:00,  1.77it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 10000/10000 [1:34:06<00:00,  1.77it/s]\n",
      "Processing grouped data filtering:   5%|▌         | 519/9997 [04:17<1:55:54,  1.36it/s]]] \n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:34:00<00:00,  1.77it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:34:45<00:00,  1.76it/s]] \n",
      "Processing grouped data filtering:  97%|█████████▋| 9659/9999 [1:34:46<01:49,  3.09it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9995/9995 [1:34:51<00:00,  1.76it/s]] \n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:35:13<00:00,  1.75it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:35:16<00:00,  1.75it/s]]]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:35:02<00:00,  1.75it/s]] \n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:35:07<00:00,  1.75it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:35:22<00:00,  1.75it/s]] \n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:35:27<00:00,  1.75it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 10000/10000 [1:35:25<00:00,  1.75it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:35:12<00:00,  1.75it/s]t]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:35:38<00:00,  1.74it/s]] \n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:35:23<00:00,  1.75it/s]] \n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:35:53<00:00,  1.74it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:35:37<00:00,  1.74it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9996/9996 [1:35:54<00:00,  1.74it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9996/9996 [1:36:08<00:00,  1.73it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 10000/10000 [1:35:56<00:00,  1.74it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:36:10<00:00,  1.73it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9996/9996 [1:35:49<00:00,  1.74it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:36:12<00:00,  1.73it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9996/9996 [1:36:28<00:00,  1.73it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:36:11<00:00,  1.73it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:36:30<00:00,  1.73it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:36:36<00:00,  1.72it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 10000/10000 [1:36:32<00:00,  1.73it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9996/9996 [1:37:01<00:00,  1.72it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:36:52<00:00,  1.72it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:37:13<00:00,  1.71it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:37:31<00:00,  1.71it/s]] \n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:38:16<00:00,  1.70it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:38:19<00:00,  1.69it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:38:17<00:00,  1.70it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 10000/10000 [1:38:27<00:00,  1.69it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9996/9996 [1:38:23<00:00,  1.69it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:39:05<00:00,  1.68it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:39:57<00:00,  1.67it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9996/9996 [1:40:30<00:00,  1.66it/s] \n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:43:19<00:00,  1.61it/s] \n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:44:56<00:00,  1.59it/s]] \n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:56:22<00:00,  1.43it/s]  \n",
      "Processing grouped data filtering: 100%|██████████| 9994/9994 [1:23:11<00:00,  2.00it/s]]  \n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:24:54<00:00,  1.96it/s]]  \n",
      "Processing grouped data filtering: 100%|██████████| 9996/9996 [1:20:38<00:00,  2.07it/s]   \n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:24:40<00:00,  1.97it/s]] \n",
      "Processing grouped data filtering: 100%|██████████| 9993/9993 [1:24:09<00:00,  1.98it/s]t]  \n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:23:18<00:00,  2.00it/s]] \n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:25:18<00:00,  1.95it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:22:20<00:00,  2.02it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:23:26<00:00,  2.00it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:27:24<00:00,  1.91it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:25:31<00:00,  1.95it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:29:09<00:00,  1.87it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 10000/10000 [1:22:42<00:00,  2.02it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:23:45<00:00,  1.99it/s]] \n",
      "Processing grouped data filtering:  91%|█████████▏| 9140/9998 [1:19:37<06:04,  2.36it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:25:49<00:00,  1.94it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:26:51<00:00,  1.92it/s]]]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:22:09<00:00,  2.03it/s]] \n",
      "Processing grouped data filtering: 100%|██████████| 10000/10000 [1:24:30<00:00,  1.97it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:26:33<00:00,  1.92it/s]] \n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:29:14<00:00,  1.87it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:26:21<00:00,  1.93it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9996/9996 [1:23:23<00:00,  2.00it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9996/9996 [1:30:26<00:00,  1.84it/s]]\n",
      "Processing grouped data filtering:  98%|█████████▊| 9788/9995 [1:21:34<02:39,  1.30it/s]] \n",
      "Processing grouped data filtering: 100%|██████████| 10000/10000 [1:25:31<00:00,  1.95it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:26:34<00:00,  1.92it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:26:40<00:00,  1.92it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:24:58<00:00,  1.96it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:29:47<00:00,  1.86it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:28:03<00:00,  1.89it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:24:51<00:00,  1.96it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9996/9996 [1:27:35<00:00,  1.90it/s]]\n",
      "Processing grouped data filtering:  95%|█████████▍| 9484/9998 [1:22:40<04:50,  1.77it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9995/9995 [1:22:58<00:00,  2.01it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9994/9994 [1:33:19<00:00,  1.78it/s]]\n",
      "Processing grouped data filtering: 100%|█████████▉| 9951/9997 [1:29:34<00:18,  2.49it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:27:26<00:00,  1.91it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:29:15<00:00,  1.87it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:27:56<00:00,  1.89it/s]]\n",
      "Processing grouped data filtering:  90%|█████████ | 9023/9996 [1:24:37<04:53,  3.31it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:29:47<00:00,  1.86it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:23:40<00:00,  1.99it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9994/9994 [1:28:29<00:00,  1.88it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 10000/10000 [1:26:22<00:00,  1.93it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:27:04<00:00,  1.91it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 10000/10000 [1:24:45<00:00,  1.97it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:26:46<00:00,  1.92it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:29:08<00:00,  1.87it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:29:48<00:00,  1.86it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9996/9996 [1:27:42<00:00,  1.90it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:29:54<00:00,  1.85it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:28:07<00:00,  1.89it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:26:25<00:00,  1.93it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9996/9996 [1:26:30<00:00,  1.93it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9996/9996 [1:27:03<00:00,  1.91it/s]  \n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:27:44<00:00,  1.90it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:27:23<00:00,  1.91it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:28:08<00:00,  1.89it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9994/9994 [1:26:41<00:00,  1.92it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:29:37<00:00,  1.86it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:25:11<00:00,  1.96it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:25:21<00:00,  1.95it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:27:19<00:00,  1.91it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9995/9995 [1:28:37<00:00,  1.88it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:35:00<00:00,  1.75it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9994/9994 [1:31:23<00:00,  1.82it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:27:12<00:00,  1.91it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9997/9997 [1:29:45<00:00,  1.86it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:30:31<00:00,  1.84it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9999/9999 [1:30:01<00:00,  1.85it/s]\n",
      "Processing grouped data filtering: 100%|██████████| 9998/9998 [1:31:15<00:00,  1.83it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 9996/9996 [1:30:22<00:00,  1.84it/s]]\n",
      "Processing grouped data filtering: 100%|██████████| 10000/10000 [1:31:24<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # actual modeling logs folder, which contains the chunks of actual_modeling_logs\n",
    "    input_folder = '/data/groupby'\n",
    "    unique_commands_path = '/data/message_counts.parquet'\n",
    "    lang_dict_path = '/data/command_dictionary.csv'\n",
    "    tool_menu_event_path = '/data/command_pairs_collections.csv'\n",
    "    output_path = '/data/aligned_logs.parquet'\n",
    "\n",
    "    file_paths = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith('.parquet')]\n",
    "    \n",
    "    process_files_in_parallel(file_paths, unique_commands_path, lang_dict_path, tool_menu_event_path, output_path, num_workers=80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412ef130",
   "metadata": {},
   "source": [
    "### Postprocessing: Remove Infrequent Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609cba27-b88e-4b0b-8fa3-59bc744c8528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "df_filtered = pd.read_parquet(output_path)\n",
    "\n",
    "message_list = df_filtered['message_eng'].to_list\n",
    "counts = Counter(df_filtered['message_eng'])\n",
    "combined_message_count = pd.DataFrame.from_dict(counts, orient='index', columns=['count'])\n",
    "combined_message_count = combined_message_count.reset_index().rename(columns={'index': 'message'})\n",
    "combined_message_count = combined_message_count.sort_values(by = 'count', ascending = False)\n",
    "\n",
    "def drop_less_commands(df, commands_set):\n",
    "    return df[~df['message_content'].isin(commands_set)]\n",
    "# Filter messages with a count less than 10 and extract their content as a list\n",
    "df_merged_deleted = combined_message_count.loc[combined_message_count['count'] < 10, 'message_content'].tolist()\n",
    "df_log_dataset_dropped = drop_less_commands(df_filtered, df_merged_deleted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f3208a",
   "metadata": {},
   "source": [
    "## Aligned Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_dataset_dropped.to_parquet('/data/aligned_logs.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
