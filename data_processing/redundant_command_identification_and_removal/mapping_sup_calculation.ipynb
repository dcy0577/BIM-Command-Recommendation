{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce82bcbb-0ae4-4117-a66f-62fbe0caf548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import string\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a788bb3",
   "metadata": {},
   "source": [
    "### Functions to calculate the **support** of the high-\\low-level command pairs. Each high-level command will form 5 high-\\low-level command pairs with it's following 5 low-level commands based on the timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb17db05-be33-4584-a768-8d8538456e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_support(df, itemsets, counts):\n",
    "    sup_list = []\n",
    "    for itemset in tqdm(itemsets, desc=\"Calculating support\"):\n",
    "        s1 = counts[itemset[0]]\n",
    "        s2 = df[df['Tool_Menu'] == itemset[0]]['UNDOs'].apply(lambda x: itemset[1] in x).sum()\n",
    "        sup_list.append({\n",
    "            'tuple_commands': itemset,\n",
    "            'tool/menu': itemset[0],\n",
    "            'event': itemset[1],\n",
    "            'sup1': s1,\n",
    "            'sup2': s2\n",
    "        })\n",
    "    return sup_list\n",
    "\n",
    "def set_tuple_commands(df):\n",
    "    C = []\n",
    "    for _, row in tqdm(df.iterrows(), desc=\"Processing to get command pairs\"):\n",
    "        high_level_message = row['Tool_Menu']\n",
    "        for item in row['UNDOs']:\n",
    "            command_set = [high_level_message, item]\n",
    "            if command_set not in C:\n",
    "                C.append(command_set)\n",
    "    return C\n",
    "\n",
    "def following_five_undo(df):\n",
    "    grouped_data = df.groupby('session_anonymized')\n",
    "    processed_data = []\n",
    "    for session_id, group_df in tqdm(grouped_data, desc=\"Processing grouped data filtering\"):\n",
    "        for index, row in (group_df.iterrows()):\n",
    "            if row['cat'] in ['Tool', 'Menu']:\n",
    "                result_dict = {\n",
    "                    'Tool_Menu': row['message_eng'],\n",
    "                    'UNDOs': None, \n",
    "                }\n",
    "                ts = row['ts']\n",
    "                # Define the range of indices for surrounding 20 rows\n",
    "                start_index = max(0, index - 10)\n",
    "                end_index = index + 10\n",
    "                # Use boolean indexing to filter rows within the desired range\n",
    "                surrounding_rows = group_df[(group_df.index >= start_index) & (group_df.index <= end_index)]\n",
    "                sub_rows = surrounding_rows[(surrounding_rows['ts'] >= ts)]\n",
    "                up_rows = surrounding_rows[(surrounding_rows['ts'] < ts)]\n",
    "                # Find the first 3 and last 1 'UNDO' action in these subsequent rows\n",
    "                undo_rows = pd.concat([sub_rows[sub_rows['cat'] == 'UNDO'].head(3),\n",
    "                                       up_rows[up_rows['cat'] == 'UNDO'].head(1)])\n",
    "\n",
    "                undo_messages = undo_rows['message_eng'].tolist()\n",
    "                result_dict['UNDOs'] = undo_messages\n",
    "                processed_data.append(result_dict)\n",
    "\n",
    "    results_df = pd.DataFrame(processed_data)\n",
    "    return results_df\n",
    "\n",
    "def get_all_files(directory_path):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        dirs.sort()\n",
    "        files.sort()\n",
    "        for file in files:\n",
    "            if file.endswith('.parquet'):\n",
    "                all_files.append(os.path.join(root, file))\n",
    "    return all_files\n",
    "\n",
    "def read_unique_commands(path):\n",
    "    df_unique_commands = pd.read_parquet(path)\n",
    "    df_unique_commands.reset_index(inplace=True)\n",
    "    return df_unique_commands\n",
    "\n",
    "def drop_less_commands(df, commands_set):\n",
    "    return df[~df['message'].isin(commands_set)]\n",
    "\n",
    "def read_language_dic(path, df):\n",
    "    language_df = pd.read_csv(path)\n",
    "    translation_dict = pd.Series(language_df.label.values, index=language_df.message).to_dict()\n",
    "    df['message_eng'] = df['message'].map(translation_dict)\n",
    "    return df.dropna(subset=['message_eng'])\n",
    "\n",
    "def contains_non_printable(text):\n",
    "    printable = set(string.printable)\n",
    "    return any(char not in printable for char in text)\n",
    "\n",
    "def process_file(file_path, file_index, unique_commands_path, lang_dict_path):\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        df_unique_commands = read_unique_commands(unique_commands_path)\n",
    "        drop_commands = df_unique_commands[df_unique_commands['count'] <= 10]['message'].tolist()\n",
    "\n",
    "        df = drop_less_commands(df, drop_commands)\n",
    "        df = read_language_dic(lang_dict_path, df)\n",
    "        df = df[~df['message_eng'].apply(contains_non_printable)]\n",
    "\n",
    "        list_df = following_five_undo(df)\n",
    "        counts = Counter(list_df['Tool_Menu'])\n",
    "\n",
    "        tuple_commands = set_tuple_commands(list_df)\n",
    "\n",
    "        sup_list = calculate_support(list_df, tuple_commands, counts)\n",
    "\n",
    "        sup_df = pd.DataFrame(sup_list)\n",
    "        sup_df['file_index'] = file_index  # Add column to mark which file the data came from\n",
    "\n",
    "        return sup_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b67d48c7-aa09-4438-9e00-8e619b60320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(path, unique_commands_path, lang_dict_path, output_path, n_jobs=80):\n",
    "    all_files = get_all_files(path)\n",
    "    print(f\"Total Parquet files found: {len(all_files)}\")\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(process_file)(file_path, idx, unique_commands_path, lang_dict_path)\n",
    "                                      for idx, file_path in enumerate(tqdm(all_files)))\n",
    "\n",
    "    # Concatenate results into a single DataFrame\n",
    "    final_result = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    final_result.to_parquet(output_path, index=False)\n",
    "    print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b286006e-c246-4066-98fb-fd8622dd04fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_path = '/data/test_data'\n",
    "    unique_commands_path = '/data/message_counts.parquet'\n",
    "    lang_dict_path = '/data/command_dictionary.csv'\n",
    "    output_path = '/data/support.parquet'\n",
    "    \n",
    "    main(data_path, unique_commands_path, lang_dict_path, output_path, n_jobs=80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
